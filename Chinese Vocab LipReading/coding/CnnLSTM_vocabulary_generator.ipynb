{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "import imutils\n",
    "import gc\n",
    "from sklearn.utils import shuffle\n",
    "from os.path import join\n",
    "from keras.utils.np_utils import *\n",
    "import keras\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "minibatch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斷詞\n",
    "vocab_200 = open('./split_data/words.txt', encoding='UTF-8-sig')\n",
    "split_word = []\n",
    "for line in vocab_200:\n",
    "    line = line.strip('\\n')\n",
    "    split_word.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做字典\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tok_path = join('split_data', 'vocabulary_tok.pickle')\n",
    "\n",
    "# saving\n",
    "if not os.path.exists(tok_path):\n",
    "    tok = Tokenizer(char_level=False)\n",
    "    tok.fit_on_texts(split_word)\n",
    "    with open(tok_path, 'wb') as handle:\n",
    "        pickle.dump(tok, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        print('create tok')\n",
    "# loading\n",
    "else:\n",
    "    with open(tok_path, 'rb') as handle:\n",
    "        tok = pickle.load(handle)\n",
    "        print('load tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tok.word_index))          # 詞彙的個數    1~200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for ii,iterm in enumerate(tok.word_index.items()):\n",
    "#     print(iterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_text(label):                       # label is list\n",
    "    words = tok.sequences_to_texts([[label[0]+1]])\n",
    "    text = words[0]\n",
    "    return(text)\n",
    "\n",
    "def text_to_labels(text):                        # text is string\n",
    "    seq = tok.texts_to_sequences([text])\n",
    "    seq = seq[0][0] - 1\n",
    "    return(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = np.load('./split_data/image_train.npy')\n",
    "image_val = np.load('./split_data/image_val.npy')\n",
    "\n",
    "f_val = open('./split_data/txt_val.txt', encoding='UTF-8-sig')\n",
    "txt_val = []\n",
    "for line in f_val:\n",
    "    line = line.strip('\\n')\n",
    "    txt_val.append(line)\n",
    "    \n",
    "f_train = open('./split_data/txt_train.txt', encoding='UTF-8-sig')\n",
    "txt_train = []\n",
    "for line in f_train:\n",
    "    line = line.strip('\\n')\n",
    "    txt_train.append(line)\n",
    "    \n",
    "txt_train = np.array(txt_train)\n",
    "txt_val = np.array(txt_val)\n",
    "\n",
    "print(image_train.shape, len(txt_train))\n",
    "print(image_val.shape, len(txt_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f_train = open('./split_data/words.txt', encoding='UTF-8-sig')\n",
    "# txt_train = []\n",
    "# for line in f_train:\n",
    "#     line = line.strip('\\n')\n",
    "#     txt_train.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_label = []\n",
    "# for i in txt_train:\n",
    "#     a = text_to_labels(i)\n",
    "#     train_label.append(a)\n",
    "# len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_labels = to_categorical(train_label, 200)\n",
    "\n",
    "# # 由one-hot转换为普通np数组\n",
    "# data = [np.argmax(one_hot)for one_hot in train_labels]\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = []\n",
    "# for i in train_label:\n",
    "#     a = labels_to_text([i])\n",
    "#     result.append(a)\n",
    "# len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, shuffle = True):\n",
    "        self.indexes = np.arange(len(txt_train))\n",
    "        self.shuffle = True\n",
    "        self.batch_size = minibatch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(image_train) / minibatch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 生成batch_size个索引\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        if (len(batch_indexes) !=  minibatch_size):\n",
    "            self.batch_size = len(batch_indexes)\n",
    "        else:\n",
    "            self.batch_size = minibatch_size\n",
    "        \n",
    "        batch_data = []\n",
    "        batch_data = image_train[batch_indexes]\n",
    "\n",
    "        batch_label = []\n",
    "        for index1 in txt_train[batch_indexes]:\n",
    "            temp = text_to_labels(index1)\n",
    "            batch_label.append(temp)\n",
    "        batch_label = np.array(batch_label)\n",
    "        batch_label = to_categorical(batch_label, 200)\n",
    "        \n",
    "        # 畫素資料浮點化以便歸一化\n",
    "        batch_data = batch_data.astype('float32')\n",
    "        batch_data /= 255\n",
    "        \n",
    "#         inputs = {'the_input': batch_data,\n",
    "#                   'the_labels': batch_label,}\n",
    "#         outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "        \n",
    "        return (batch_data, batch_label)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        #在每一次epoch结束是否需要进行一次随机，重新随机一下index\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, shuffle = True):\n",
    "        self.indexes = np.arange(len(txt_val))\n",
    "        self.shuffle = True\n",
    "        self.batch_size = minibatch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(image_val) / minibatch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 生成batch_size个索引\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        if (len(batch_indexes) !=  minibatch_size):\n",
    "            self.batch_size = len(batch_indexes)\n",
    "        else:\n",
    "            self.batch_size = minibatch_size\n",
    "        GRU\n",
    "        batch_data = []\n",
    "        batch_data = image_val[batch_indexes]\n",
    "\n",
    "        batch_label = []\n",
    "        for index1 in txt_val[batch_indexes]:\n",
    "            temp = text_to_labels(index1)\n",
    "            batch_label.append(temp)\n",
    "        batch_label = np.array(batch_label)\n",
    "        batch_label = to_categorical(batch_label, 200)\n",
    "        \n",
    "        # 畫素資料浮點化以便歸一化\n",
    "        batch_data = batch_data.astype('float32')\n",
    "        batch_data /= 255\n",
    "        \n",
    "#         inputs = {'the_input': batch_data,\n",
    "#                   'the_labels': batch_label,}\n",
    "#         outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "        \n",
    "        return (batch_data, batch_label)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        #在每一次epoch结束是否需要进行一次随机，重新随机一下index\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, SpatialDropout3D\n",
    "from keras.layers import Convolution3D, MaxPooling3D\n",
    "from keras.layers.convolutional import Conv3D, ZeroPadding3D\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from multiprocessing import set_start_method, Pool\n",
    "set_start_method('forkserver')\n",
    "from Word_Error_Rate import WordErrorRate\n",
    "from wer import *\n",
    "from decoders import Decoder\n",
    "# from error_rates import ErrorRates\n",
    "# out_size = len(OneWord)+1               # add ctc blank char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MODEL(object):\n",
    "    def __init__(self, img_c=3, img_w=100, img_h=50, frames_n=77, output_size=len(tok.word_index)):\n",
    "        self.img_c = img_c\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.frames_n = frames_n\n",
    "#         self.absolute_max_string_len = absolute_max_string_len\n",
    "        self.output_size = output_size\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.input_data = Input(name='the_input', shape=(77,50,100,3), dtype='float32')\n",
    "        \n",
    "        self.zero1 = ZeroPadding3D(padding=(1, 2, 2), name='zero1')(self.input_data)\n",
    "        self.conv1 = Conv3D(32, (3, 5, 5), strides=(1, 2, 2), kernel_initializer='he_normal', name='conv1')(self.zero1)\n",
    "        self.batc1 = BatchNormalization(name='batc1')(self.conv1)\n",
    "        self.actv1 = Activation('relu', name='actv1')(self.batc1)\n",
    "        self.drop1 = SpatialDropout3D(0.5)(self.actv1)\n",
    "        self.maxp1 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max1')(self.drop1)\n",
    "\n",
    "        self.zero2 = ZeroPadding3D(padding=(1, 2, 2), name='zero2')(self.maxp1)\n",
    "        self.conv2 = Conv3D(64, (3, 5, 5), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv2')(self.zero2)\n",
    "        self.batc2 = BatchNormalization(name='batc2')(self.conv2)\n",
    "        self.actv2 = Activation('relu', name='actv2')(self.batc2)\n",
    "        self.drop2 = SpatialDropout3D(0.5)(self.actv2)\n",
    "        self.maxp2 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max2')(self.drop2)\n",
    "\n",
    "        self.zero3 = ZeroPadding3D(padding=(1, 1, 1), name='zero3')(self.maxp2)\n",
    "        self.conv3 = Conv3D(96, (3, 3, 3), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv3')(self.zero3)\n",
    "        self.batc3 = BatchNormalization(name='batc3')(self.conv3)\n",
    "        self.actv3 = Activation('relu', name='actv3')(self.batc3)\n",
    "        self.drop3 = SpatialDropout3D(0.5)(self.actv3)\n",
    "        self.maxp3 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max3')(self.drop3)\n",
    "\n",
    "        self.resh1 = TimeDistributed(Flatten())(self.maxp3)\n",
    "\n",
    "        self.gru_1 = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(self.resh1)\n",
    "        self.gru_2 = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru2'), merge_mode='concat')(self.gru_1)\n",
    "\n",
    "        self.resh2 = Flatten()(self.gru_2)\n",
    "        \n",
    "        # transforms RNN output to character activations:\n",
    "        self.dense1 = Dense(self.output_size, kernel_initializer='he_normal', name='dense1')(self.resh2)\n",
    "\n",
    "        self.y_pred = Activation('softmax', name='softmax')(self.dense1)\n",
    "\n",
    "#         self.labels = Input(name='the_labels', shape=[self.absolute_max_string_len], dtype='float32')\n",
    "#         self.input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "#         self.label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "#         self.loss_out = CTC('ctc', [self.y_pred, self.labels, self.input_length, self.label_length])\n",
    "\n",
    "        self.model = Model(inputs = self.input_data, outputs = self.y_pred)\n",
    "        \n",
    "    def summary(self):\n",
    "        Model(inputs=self.input_data, outputs=self.y_pred).summary()\n",
    "\n",
    "#     def predict(self, input_batch):\n",
    "#         return self.test_function([input_batch, 0])[0]  # the first 0 indicates test\n",
    "\n",
    "#     @property\n",
    "#     def test_function(self):\n",
    "#         # captures output of softmax so we can decode the output during visualization\n",
    "#         return K.function([self.input_data, K.learning_phase()], [self.y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "# help(TimeDistributed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_size = 200\n",
    "model = MODEL(img_c=3,img_w=100,img_h=50,frames_n=77,output_size=len(tok.word_index))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = TrainDataGenerator()\n",
    "val_generator = ValDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# MODEL.model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复模型结构及权重\n",
    "model.model.load_weights('./weight/lipnet_50_vocab.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder = Decoder(greedy=True,beam_width=100,postprocessors=[labels_to_text])\n",
    "# error_rates = ErrorRates(lipnet, val_generator, decoder, 256)\n",
    "# WordError_Rate = WordErrorRate(lipnet, val_generator, decoder, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.fit_generator(generator = train_generator,\n",
    "                           validation_data = val_generator,\n",
    "                           epochs = 100,                 \n",
    "#                            callbacks = [WordError_Rate],\n",
    "                           verbose = 1,\n",
    "                           max_q_size = 5,\n",
    "                           shuffle = True\n",
    "#                            workers = 2,\n",
    "#                            pickle_safe=True,\n",
    "#                            use_multiprocessing = True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型结构及权重\n",
    "model.model.save('./weight/lipnet_150_vocab.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复模型结构及权重\n",
    "# lipnet.model.load_weights('./weight/368-overlap-6.h5')\n",
    "lipnet.model.load_weights('./weight/lipnet_1000_vocab.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = 0             # 0, 120, 240\n",
    "pred_batch = 150\n",
    "def predicts(data, input_length):\n",
    "    batch_data = []\n",
    "    batch_data = data[initial:initial + pred_batch]\n",
    "    \n",
    "    # 畫素資料浮點化以便歸一化\n",
    "    batch_data = batch_data.astype('float32')\n",
    "    batch_data /= 255\n",
    "    \n",
    "    batch_input_length = []\n",
    "    batch_input_length = input_length[initial:initial + pred_batch]\n",
    "    \n",
    "    return (batch_data, batch_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data, pred_input_length = predicts(image_val, val_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lipnet.predict(pred_data)\n",
    "print(y_pred.shape)\n",
    "# print(pred_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctc decode\n",
    "r = K.ctc_decode(y_pred, pred_input_length, greedy = True, beam_width=100, top_paths=1)\n",
    "r1 = K.get_value(r[0][0])\n",
    "# print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder(decoded, **kwargs):\n",
    "    postprocessors = kwargs.get('postprocessors', [])\n",
    "    preprocessed = []\n",
    "    for output in decoded:\n",
    "        out = output\n",
    "        for postprocessor in postprocessors:\n",
    "            out = postprocessor(out)\n",
    "        preprocessed.append(out)\n",
    "    return(preprocessed)\n",
    "result = Decoder(r1, postprocessors=[labels_to_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result:\n",
    "    print('Predict label:', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in txt_val[initial:initial + pred_batch]:\n",
    "    print('True label:', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def GetEditDistance(str1, str2):\n",
    "    leven_cost = 0\n",
    "    s = difflib.SequenceMatcher(None, str1, str2)\n",
    "    for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "        #print('{:7} a[{}: {}] --> b[{}: {}] {} --> {}'.format(tag, i1, i2, j1, j2, str1[i1: i2], str2[j1: j2]))\n",
    "        if tag == 'replace':\n",
    "            leven_cost += max(i2-i1, j2-j1)\n",
    "        elif tag == 'insert':\n",
    "            leven_cost += (j2-j1)\n",
    "        elif tag == 'delete':\n",
    "            leven_cost += (i2-i1)\n",
    "    return leven_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cer(predict, label, label_length):\n",
    "    # print(data)\n",
    "    # mean_length = np.mean([len(d[1]) for d in data])\n",
    "    cha_num = 0\n",
    "    cha_error_num = 0\n",
    "\n",
    "    for i in range(len(predict)):\n",
    "        cha_edit_distance = GetEditDistance(str(predict[i]), str(label[i]))\n",
    "        cha_num = cha_num + label_length[i]\n",
    "\n",
    "        if(cha_edit_distance <= label_length[i]):\n",
    "            cha_error_num += cha_edit_distance\n",
    "        else:\n",
    "            cha_error_num += label_length[i]\n",
    "\n",
    "    return (cha_error_num / cha_num) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer(predict, label, label_length):\n",
    "    # print(data)\n",
    "    # mean_length = np.mean([len(d[1].split()) for d in data])\n",
    "    words_num = 0\n",
    "    word_error_num = 0\n",
    "\n",
    "    for i in range(len(predict)):\n",
    "        word_edit_distance = chinese_wer_sentence(str(predict[i]), str(label[i]))\n",
    "        words_num = words_num + label_length[i]\n",
    "\n",
    "        if(word_edit_distance <= label_length[i]):\n",
    "            word_error_num += word_edit_distance\n",
    "        else:\n",
    "            word_error_num += label_length[i]\n",
    "\n",
    "    return (word_error_num / words_num) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Letter_length = []\n",
    "Word_length = []\n",
    "for i in txt_val[initial:initial + pred_batch]:\n",
    "    Letter_length.append(len(i))\n",
    "    Word_length.append(len(jieba.lcut(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = calculate_wer(result, txt_val[initial:initial + pred_batch], Word_length)\n",
    "cer = calculate_cer(result, txt_val[initial:initial + pred_batch], Letter_length)\n",
    "print(\"wer: \" + str(wer))\n",
    "print(\"cer: \" + str(cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
