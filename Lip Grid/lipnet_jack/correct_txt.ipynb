{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "import imutils\n",
    "import gc\n",
    "label_max_len = 32\n",
    "import re\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_val = open('./data_split/overlap/txt_overlap_val.txt')\n",
    "txt_overlap_val = []\n",
    "for line in f_val:\n",
    "    line = line.strip('\\n')\n",
    "    txt_overlap_val.append(line)\n",
    "    \n",
    "f_train = open('./data_split/overlap/txt_overlap_train.txt')\n",
    "txt_overlap_train = []\n",
    "for line in f_train:\n",
    "    line = line.strip('\\n')\n",
    "    txt_overlap_train.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def untokenize(words):\n",
    "    \"\"\"\n",
    "    Untokenizing a text undoes the tokenizing operation, restoring\n",
    "    punctuation and spaces to the places that people expect them to be.\n",
    "    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n",
    "    except for line breaks.\n",
    "    \"\"\"\n",
    "    text = ' '.join(words)\n",
    "    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n",
    "    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n",
    "    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n",
    "    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n",
    "    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n",
    "         \"can not\", \"cannot\")\n",
    "    step6 = step5.replace(\" ` \", \" '\")\n",
    "    return step6.strip()\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/367155/splitting-a-string-into-words-and-punctuation\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "\n",
    "def words(text):\n",
    "    return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "def P(word, N=None):\n",
    "    \"Probability of `word`.\"\n",
    "    if N is None:\n",
    "        N = sum(dictionary.values())\n",
    "    return dictionary[word] / N\n",
    "\n",
    "def correction(word):\n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word):\n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words):\n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in dictionary)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n",
    "\n",
    "# Correct words\n",
    "def corrections(words):\n",
    "    return [correction(word) for word in words]\n",
    "\n",
    "# Correct sentence\n",
    "def sentence(sentence):\n",
    "    return untokenize(corrections(tokenize(sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './grid.txt'\n",
    "dictionary = Counter(list(string.punctuation) + words(open(path).read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(txt_overlap_train)):\n",
    "    temp = sentence(txt_overlap_train[i])\n",
    "    txt_overlap_train[i] = temp\n",
    "    \n",
    "for i in range(len(txt_overlap_val)):\n",
    "    temp = sentence(txt_overlap_val[i])\n",
    "    txt_overlap_val[i] = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save txt file\n",
    "with open('./data_split/overlap/correct_txt_overlap_val.txt', 'w') as txt_val:\n",
    "    for txt in txt_overlap_val:\n",
    "        txt_val.write(txt)\n",
    "        txt_val.write('\\n')\n",
    "\n",
    "with open('./data_split/overlap/correct_txt_overlap_train.txt', 'w') as txt_train:\n",
    "    for txt in txt_overlap_train:\n",
    "        txt_train.write(txt)\n",
    "        txt_train.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
