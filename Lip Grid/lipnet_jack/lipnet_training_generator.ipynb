{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "import imutils\n",
    "import gc\n",
    "from sklearn.utils import shuffle\n",
    "import keras\n",
    "import random\n",
    "import math\n",
    "label_max_len = 37\n",
    "minibatch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_labels(text):\n",
    "    ret = []\n",
    "    for char in text:\n",
    "        if char >= 'a' and char <= 'z':\n",
    "            ret.append(ord(char) - ord('a'))\n",
    "        elif char == ' ':\n",
    "            ret.append(26)\n",
    "    return ret\n",
    "\n",
    "def labels_to_text(labels):\n",
    "    # 26 is space, 27 is CTC blank char\n",
    "    text = ''\n",
    "    for c in labels:\n",
    "        if c >= 0 and c < 26:\n",
    "            text += chr(c + ord('a'))\n",
    "        elif c == 26:\n",
    "            text += ' '\n",
    "    return text\n",
    "\n",
    "def get_padded_label(label):\n",
    "    padding = np.ones((label_max_len-len(label))) * 28        # -1  \n",
    "    return np.concatenate((np.array(label), padding), axis=0)\n",
    "\n",
    "# def get_unpadded_label(label):\n",
    "#     padding = np.ones((0)) * -1\n",
    "#     return np.concatenate((np.array(label), padding), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_unseen_train = np.load('./data_split/unseen/image_unseen_train.npy')\n",
    "image_unseen_val = np.load('./data_split/unseen/image_unseen_val.npy')\n",
    "\n",
    "f_val = open('./data_split/unseen/correct_txt_unseen_val.txt')\n",
    "txt_unseen_val = []\n",
    "for line in f_val:\n",
    "    line = line.strip('\\n')\n",
    "    txt_unseen_val.append(line)\n",
    "    \n",
    "f_train = open('./data_split/unseen/correct_txt_unseen_train.txt')\n",
    "txt_unseen_train = []\n",
    "for line in f_train:\n",
    "    line = line.strip('\\n')\n",
    "    txt_unseen_train.append(line)\n",
    "    \n",
    "train_label_length = []\n",
    "train_input_length = []\n",
    "for i in txt_unseen_train:\n",
    "    train_label_length.append(len(i))\n",
    "    train_input_length.append(75)\n",
    "    \n",
    "val_label_length = []\n",
    "val_input_length = []\n",
    "for i in txt_unseen_val:\n",
    "    val_label_length.append(len(i))\n",
    "    val_input_length.append(75)\n",
    "    \n",
    "train_label_length = np.array(train_label_length)\n",
    "train_input_length = np.array(train_input_length)\n",
    "val_label_length = np.array(val_label_length)\n",
    "val_input_length = np.array(val_input_length)\n",
    "txt_unseen_train = np.array(txt_unseen_train)\n",
    "txt_unseen_val = np.array(txt_unseen_val)\n",
    "\n",
    "print(image_unseen_train.shape, txt_unseen_train.shape)\n",
    "print(image_unseen_val.shape, txt_unseen_val.shape)\n",
    "print(train_label_length.shape, train_input_length.shape)\n",
    "print(val_label_length.shape, val_input_length.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, shuffle = True):\n",
    "        self.indexes = np.arange(len(txt_unseen_train))\n",
    "        self.shuffle = True\n",
    "        self.batch_size = minibatch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(image_unseen_train) / minibatch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 生成batch_size个索引\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        if (len(batch_indexes) !=  minibatch_size):\n",
    "            self.batch_size = len(batch_indexes)\n",
    "        else:\n",
    "            self.batch_size = minibatch_size\n",
    "        \n",
    "        batch_data = []\n",
    "        batch_data = image_unseen_train[batch_indexes]\n",
    "\n",
    "        batch_label = []\n",
    "        for index1 in txt_unseen_train[batch_indexes]:\n",
    "            temp = text_to_labels(index1)\n",
    "            temp = get_padded_label(temp)\n",
    "#             temp = get_unpadded_label(temp)\n",
    "            batch_label.append(temp)\n",
    "        batch_label = np.array(batch_label)\n",
    "        \n",
    "        batch_input_length = []\n",
    "        batch_input_length = train_input_length[batch_indexes]\n",
    "        batch_label_length = []\n",
    "        batch_label_length = train_label_length[batch_indexes]\n",
    "        \n",
    "        # 畫素資料浮點化以便歸一化\n",
    "        batch_data = batch_data.astype('float32')\n",
    "        batch_data /= 255\n",
    "        \n",
    "        inputs = {'the_input': batch_data,\n",
    "                  'the_labels': batch_label,\n",
    "                  'input_length': batch_input_length,\n",
    "                  'label_length': batch_label_length}\n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "        \n",
    "        return (inputs, outputs)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        #在每一次epoch结束是否需要进行一次随机，重新随机一下index\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, shuffle = True):\n",
    "        self.indexes = np.arange(len(txt_unseen_val))\n",
    "        self.shuffle = True\n",
    "        self.batch_size = minibatch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(image_unseen_val) / minibatch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 生成batch_size个索引\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        if (len(batch_indexes) !=  minibatch_size):\n",
    "            self.batch_size = len(batch_indexes)\n",
    "        else:\n",
    "            self.batch_size = minibatch_size\n",
    "        \n",
    "        batch_data = []\n",
    "        batch_data = image_unseen_val[batch_indexes]\n",
    "\n",
    "        batch_label = []\n",
    "        for index1 in txt_unseen_val[batch_indexes]:\n",
    "            temp = text_to_labels(index1)\n",
    "            temp = get_padded_label(temp)\n",
    "#             temp = get_unpadded_label(temp)\n",
    "            batch_label.append(temp)\n",
    "        batch_label = np.array(batch_label)\n",
    "        \n",
    "        batch_input_length = []\n",
    "        batch_input_length = val_input_length[batch_indexes]\n",
    "        batch_label_length = []\n",
    "        batch_label_length = val_label_length[batch_indexes]\n",
    "        \n",
    "        # 畫素資料浮點化以便歸一化\n",
    "        batch_data = batch_data.astype('float32')\n",
    "        batch_data /= 255\n",
    "        \n",
    "        inputs = {'the_input': batch_data,\n",
    "                  'the_labels': batch_label,\n",
    "                  'input_length': batch_input_length,\n",
    "                  'label_length': batch_label_length}\n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "        \n",
    "        return (inputs, outputs)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        #在每一次epoch结束是否需要进行一次随机，重新随机一下index\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, SpatialDropout3D\n",
    "from keras.layers import Convolution3D, MaxPooling3D\n",
    "from keras.layers.convolutional import Conv3D, ZeroPadding3D\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from multiprocessing import set_start_method, Pool\n",
    "set_start_method('forkserver')\n",
    "from decoders import Decoder\n",
    "# from callbacks import Statistics, Visualize\n",
    "from spell import Spell\n",
    "# from error_rates import ErrorRates\n",
    "from Word_Error_Rate import WordErrorRate\n",
    "from wer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # From Keras example image_ocr.py:\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "    # y_pred = y_pred[:, 2:, :]\n",
    "    y_pred = y_pred[:, :, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "def CTC(name, args):\n",
    "    return Lambda(ctc_lambda_func, output_shape=(1,), name=name)(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipNet(object):\n",
    "    def __init__(self, img_c=3, img_w=100, img_h=50, frames_n=75, absolute_max_string_len=37, output_size=28):\n",
    "        self.img_c = img_c\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.frames_n = frames_n\n",
    "        self.absolute_max_string_len = absolute_max_string_len\n",
    "        self.output_size = output_size\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.input_data = Input(name='the_input', shape=(75,50,100,3), dtype='float32')\n",
    "        \n",
    "        self.zero1 = ZeroPadding3D(padding=(1, 2, 2), name='zero1')(self.input_data)\n",
    "        self.conv1 = Conv3D(32, (3, 5, 5), strides=(1, 2, 2), kernel_initializer='he_normal', name='conv1')(self.zero1)\n",
    "        self.batc1 = BatchNormalization(name='batc1')(self.conv1)\n",
    "        self.actv1 = Activation('relu', name='actv1')(self.batc1)\n",
    "        self.drop1 = SpatialDropout3D(0.5)(self.actv1)\n",
    "        self.maxp1 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max1')(self.drop1)\n",
    "\n",
    "        self.zero2 = ZeroPadding3D(padding=(1, 2, 2), name='zero2')(self.maxp1)\n",
    "        self.conv2 = Conv3D(64, (3, 5, 5), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv2')(self.zero2)\n",
    "        self.batc2 = BatchNormalization(name='batc2')(self.conv2)\n",
    "        self.actv2 = Activation('relu', name='actv2')(self.batc2)\n",
    "        self.drop2 = SpatialDropout3D(0.5)(self.actv2)\n",
    "        self.maxp2 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max2')(self.drop2)\n",
    "\n",
    "        self.zero3 = ZeroPadding3D(padding=(1, 1, 1), name='zero3')(self.maxp2)\n",
    "        self.conv3 = Conv3D(96, (3, 3, 3), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv3')(self.zero3)\n",
    "        self.batc3 = BatchNormalization(name='batc3')(self.conv3)\n",
    "        self.actv3 = Activation('relu', name='actv3')(self.batc3)\n",
    "        self.drop3 = SpatialDropout3D(0.5)(self.actv3)\n",
    "        self.maxp3 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max3')(self.drop3)\n",
    "\n",
    "        self.resh1 = TimeDistributed(Flatten())(self.maxp3)\n",
    "\n",
    "        self.gru_1 = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(self.resh1)\n",
    "        self.gru_2 = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru2'), merge_mode='concat')(self.gru_1)\n",
    "\n",
    "        # transforms RNN output to character activations:\n",
    "        self.dense1 = Dense(self.output_size, kernel_initializer='he_normal', name='dense1')(self.gru_2)\n",
    "\n",
    "        self.y_pred = Activation('softmax', name='softmax')(self.dense1)\n",
    "\n",
    "        self.labels = Input(name='the_labels', shape=[self.absolute_max_string_len], dtype='float32')\n",
    "        self.input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "        self.label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "        self.loss_out = CTC('ctc', [self.y_pred, self.labels, self.input_length, self.label_length])\n",
    "\n",
    "        self.model = Model(inputs=[self.input_data, self.labels, self.input_length, self.label_length], outputs=self.loss_out)\n",
    "        \n",
    "    def summary(self):\n",
    "        Model(inputs=self.input_data, outputs=self.y_pred).summary()\n",
    "\n",
    "    def predict(self, input_batch):\n",
    "        return self.test_function([input_batch, 0])[0]  # the first 0 indicates test\n",
    "\n",
    "    @property\n",
    "    def test_function(self):\n",
    "        # captures output of softmax so we can decode the output during visualization\n",
    "        return K.function([self.input_data, K.learning_phase()], [self.y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipnet = LipNet(img_c=3, img_w=100, img_h=50, frames_n=75, absolute_max_string_len=label_max_len, output_size=28)\n",
    "lipnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = TrainDataGenerator()\n",
    "val_generator = ValDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "lipnet.model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复模型结构及权重\n",
    "# lipnet.model.load_weights('./weight/unseen-100.h5')\n",
    "# lipnet.model.load_weights('./weight/unseen-weights178.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = Spell('./grid1.txt')\n",
    "decoder = Decoder(greedy=True,beam_width=200,postprocessors=[labels_to_text,spell.sentence])\n",
    "# statistics  = Statistics(lipnet, val_generator, decoder, 256)\n",
    "# visualize = Visualize('./unseen_visual', lipnet, val_batch_gen(image_unseen_val, txt_unseen_val, val_input_length, val_input_length, minibatch_size)\n",
    "#                      , decoder, num_display_sentences=minibatch_size)\n",
    "# error_rates = ErrorRates(lipnet, val_generator, decoder, minibatch_size)  # minibatch_size = 50\n",
    "WordError_Rate = WordErrorRate(lipnet, val_generator, decoder, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipnet.model.fit_generator(generator = train_generator,\n",
    "                           validation_data = val_generator,\n",
    "                           epochs = 50,\n",
    "                           callbacks = [WordError_Rate],\n",
    "                           verbose = 1,\n",
    "                           max_q_size = 5,\n",
    "                           shuffle = True\n",
    "#                            workers = 2,\n",
    "#                            pickle_safe=True,\n",
    "#                            use_multiprocessing = True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型结构及权重\n",
    "# lipnet.model.save('./weight/unseen-150.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复模型结构及权重\n",
    "# lipnet.model.load_weights('./weight/178-unseen-2.h5')\n",
    "lipnet.model.load_weights('./weight/unseen-200.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = 0\n",
    "pred_batch = 100\n",
    "def predicts(data, input_length):\n",
    "    batch_data = []\n",
    "    batch_data = data[initial:initial + pred_batch]\n",
    "    \n",
    "    # 畫素資料浮點化以便歸一化\n",
    "    batch_data = batch_data.astype('float32')\n",
    "    batch_data /= 255\n",
    "    \n",
    "    batch_input_length = []\n",
    "    batch_input_length = input_length[initial:initial + pred_batch]\n",
    "    \n",
    "    return (batch_data, batch_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data, pred_input_length = predicts(image_unseen_val, val_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lipnet.predict(pred_data)\n",
    "print(y_pred.shape)\n",
    "print(pred_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ctc decode\n",
    "# r = K.ctc_decode(y_pred, pred_input_length, greedy = True, beam_width=100, top_paths=1)\n",
    "# r1 = K.get_value(r[0][0])\n",
    "# print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = decoder.decode(y_pred, pred_input_length)\n",
    "# for i in result:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in txt_unseen_val[initial:initial + pred_batch]:\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate (wer, cer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def GetEditDistance(str1, str2):\n",
    "    leven_cost = 0\n",
    "    s = difflib.SequenceMatcher(None, str1, str2)\n",
    "    for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "        #print('{:7} a[{}: {}] --> b[{}: {}] {} --> {}'.format(tag, i1, i2, j1, j2, str1[i1: i2], str2[j1: j2]))\n",
    "        if tag == 'replace':\n",
    "            leven_cost += max(i2-i1, j2-j1)\n",
    "        elif tag == 'insert':\n",
    "            leven_cost += (j2-j1)\n",
    "        elif tag == 'delete':\n",
    "            leven_cost += (i2-i1)\n",
    "    return leven_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cer(predict, label, label_length):\n",
    "    # print(data)\n",
    "    # mean_length = np.mean([len(d[1]) for d in data])\n",
    "    cha_num = 0\n",
    "    cha_error_num = 0\n",
    "\n",
    "    for i in range(len(predict)):\n",
    "        cha_edit_distance = GetEditDistance(str(predict[i]), str(label[i]))\n",
    "        cha_num = cha_num + label_length[i]\n",
    "\n",
    "        if(cha_edit_distance <= label_length[i]):\n",
    "            cha_error_num += cha_edit_distance\n",
    "        else:\n",
    "            cha_error_num += label_length[i]\n",
    "\n",
    "    return (cha_error_num / cha_num) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer(predict, label, label_length):\n",
    "    # print(data)\n",
    "    # mean_length = np.mean([len(d[1].split()) for d in data])\n",
    "    words_num = 0\n",
    "    word_error_num = 0\n",
    "\n",
    "    for i in range(len(predict)):\n",
    "        word_edit_distance = wer_sentence(str(predict[i]), str(label[i]))\n",
    "        words_num = words_num + label_length[i]\n",
    "\n",
    "        if(word_edit_distance <= label_length[i]):\n",
    "            word_error_num += word_edit_distance\n",
    "        else:\n",
    "            word_error_num += label_length[i]\n",
    "\n",
    "    return (word_error_num / words_num) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Letter_length = []\n",
    "Word_length = []\n",
    "for i in txt_unseen_val[initial:initial + pred_batch]:\n",
    "    Letter_length.append(len(i))\n",
    "    Word_length.append(len(i.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = calculate_wer(result, txt_unseen_val[initial:initial + pred_batch], Word_length)\n",
    "cer = calculate_cer(result, txt_unseen_val[initial:initial + pred_batch], Letter_length)\n",
    "print(\"wer: \" + str(wer))\n",
    "print(\"cer: \" + str(cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"1 23 456\"\n",
    "b = \"1 456 23\"\n",
    "c = \"我愛你\"\n",
    "d = \"你愛我\"\n",
    "cer = GetEditDistance(c, d)\n",
    "wer = wer_sentence(c,d)\n",
    "print(cer, wer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
