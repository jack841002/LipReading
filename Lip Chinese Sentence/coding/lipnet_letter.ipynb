{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "import imutils\n",
    "import gc\n",
    "from sklearn.utils import shuffle\n",
    "from os.path import join\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import keras\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "label_max_len = 14\n",
    "minibatch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_len(str):                        # 中文字的byte=2, 英文字的byte=1\n",
    "#     row_l=len(str)\n",
    "#     utf8_l=len(str.encode('utf-8'))\n",
    "#     return (utf8_l-row_l)/2+row_l\n",
    "    return(len(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def ctc_len(label):\n",
    "#     add_len = 0\n",
    "#     label_len = len(label)\n",
    "#     for i in range(label_len - 1):\n",
    "#         if label[i] == label[i + 1]:\n",
    "#             add_len += 1  # 这里+1是因为ctc会在重复字符之间填充blank\n",
    "#     return label_len + add_len\n",
    "\n",
    "# label = \"中aa文\"\n",
    "# target_length = len(label)\n",
    "# target_ctc_len = ctc_len(label)\n",
    "# print(target_length, target_ctc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_train = np.load('./split_data/image_train.npy')\n",
    "image_val = np.load('./split_data/image_val.npy')\n",
    "\n",
    "f_val = open('./split_data/txt_val.txt')\n",
    "txt_val = []\n",
    "for line in f_val:\n",
    "    line = line.strip('\\n')\n",
    "    txt_val.append(line)\n",
    "    \n",
    "f_train = open('./split_data/txt_train.txt')\n",
    "txt_train = []\n",
    "for line in f_train:\n",
    "    line = line.strip('\\n')\n",
    "    txt_train.append(line)\n",
    "    \n",
    "train_label_length = []\n",
    "for i in txt_train:\n",
    "    sentence_len = str_len(i)\n",
    "    train_label_length.append(sentence_len)\n",
    "\n",
    "train_input_length = []\n",
    "for video in image_train:\n",
    "    num = 0\n",
    "    for pic in video:\n",
    "        if np.where(pic != 0)[0].shape[0] != 0:             # 判断是否是零矩阵\n",
    "            num = num + 1\n",
    "    train_input_length.append(num)\n",
    "    \n",
    "val_label_length = []\n",
    "for i in txt_val:\n",
    "    sentence_len = str_len(i)\n",
    "    val_label_length.append(sentence_len)\n",
    "\n",
    "val_input_length = []\n",
    "for video in image_val:\n",
    "    num = 0\n",
    "    for pic in video:\n",
    "        if np.where(pic != 0)[0].shape[0] != 0:             # 判断是否是零矩阵\n",
    "            num = num + 1\n",
    "    val_input_length.append(num)\n",
    "    \n",
    "train_label_length = np.array(train_label_length)\n",
    "train_input_length = np.array(train_input_length)\n",
    "val_label_length = np.array(val_label_length)\n",
    "val_input_length = np.array(val_input_length)\n",
    "txt_train = np.array(txt_train)\n",
    "txt_val = np.array(txt_val)\n",
    "\n",
    "print(image_train.shape, txt_train.shape)\n",
    "print(image_val.shape, txt_val.shape)\n",
    "print(train_label_length.shape, train_input_length.shape)\n",
    "print(val_label_length.shape, val_input_length.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算label的最大長度和平均長度\n",
    "maxLen = 0\n",
    "summ = 0\n",
    "for i in txt_train:\n",
    "    if str_len(i) > maxLen:\n",
    "        maxLen = str_len(i)\n",
    "    summ = summ + str_len(i)\n",
    "print(maxLen, summ/len(txt_train))\n",
    "\n",
    "maxLen1 = 0\n",
    "summ1 = 0\n",
    "for i in txt_val:\n",
    "    if str_len(i) > maxLen1:\n",
    "        maxLen1 = str_len(i)\n",
    "    summ1 = summ1 + str_len(i)\n",
    "print(maxLen1, summ1/len(txt_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斷詞\n",
    "hyphenation = open('./split_data/words.txt')\n",
    "split_word = []\n",
    "for line in hyphenation:\n",
    "    line = line.strip('\\n')\n",
    "    split_word.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做字典\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tok_path = join('split_data', 'letter_tok.pickle')\n",
    "\n",
    "# saving\n",
    "if not os.path.exists(tok_path):\n",
    "    tok = Tokenizer(char_level=True)\n",
    "    tok.fit_on_texts(split_word)\n",
    "    with open(tok_path, 'wb') as handle:\n",
    "        pickle.dump(tok, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "        print('create tok')\n",
    "# loading\n",
    "else:\n",
    "    with open(tok_path, 'rb') as handle:\n",
    "        tok = pickle.load(handle)\n",
    "        print('load tok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tok.word_index))        # 字的種類個數"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 使用word_index屬性可以看到每個詞對應的編碼\n",
    "# ## 使用word_counts屬性可以看到每個詞對應的頻數\n",
    "# for ii,iterm in enumerate(tok.word_index.items()):\n",
    "#     print(iterm)\n",
    "# print(\"===================\")  \n",
    "# for ii,iterm in enumerate(tok.word_counts.items()):\n",
    "#     print(iterm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pad_seq = []\n",
    "# for train_text in txt_train:\n",
    "#     train_seq = tok.texts_to_sequences([train_text])\n",
    "#     train_seq = train_seq[0]\n",
    "#     padding = np.ones((label_max_len-len(train_seq))) * 0   \n",
    "#     temp = np.concatenate((np.array(train_seq), padding), axis=0)\n",
    "#     train_pad_seq.append(temp)\n",
    "    \n",
    "# val_pad_seq = []\n",
    "# for val_text in txt_val:\n",
    "#     val_seq = tok.texts_to_sequences([val_text])\n",
    "#     pad_seq = pad_sequences(val_seq, maxlen=label_max_len, padding='post', truncating='post')\n",
    "#     pad_seq = pad_seq[0]\n",
    "#     val_pad_seq.append(pad_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_text(label):\n",
    "    label = list(label[label>0])\n",
    "    words = tok.sequences_to_texts([label])\n",
    "    text = words[0].replace(\" \", \"\")\n",
    "    return(text)\n",
    "\n",
    "def text_to_labels(text):\n",
    "    seq = tok.texts_to_sequences([text])\n",
    "    seq = seq[0]\n",
    "    return(seq)\n",
    "    \n",
    "# def get_padded_label(seq):\n",
    "#     pad_seq = pad_sequences(seq, maxlen=label_max_len, padding='post', truncating='post')\n",
    "#     pad_seq = pad_seq[0]\n",
    "#     return(pad_seq)\n",
    "\n",
    "def get_padded_label(seq):\n",
    "    padding = np.ones((label_max_len-len(seq))) * 0   \n",
    "    return np.concatenate((np.array(seq), padding), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneWord = []\n",
    "# for sentence in txt_train:\n",
    "#     for word in sentence:\n",
    "#         OneWord.append(word)\n",
    "        \n",
    "# for sentence in txt_val:\n",
    "#     for word in sentence:\n",
    "#         OneWord.append(word)\n",
    "\n",
    "# OneWord = set(OneWord)\n",
    "# OneWord = list(OneWord)\n",
    "# # OneWord.append(\" \")                # add space to list\n",
    "# print(len(OneWord))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_to_labels(text):\n",
    "#     ret = []\n",
    "#     for char in text:\n",
    "#         index = OneWord.index(char) # + 1 \n",
    "#         ret.append(index)\n",
    "#     return ret\n",
    "\n",
    "# def labels_to_text(labels):\n",
    "#     text = ''\n",
    "#     for num in labels:\n",
    "#         temp = OneWord[num]\n",
    "#         text = text + temp\n",
    "#     return text\n",
    "\n",
    "# def get_padded_label(label):\n",
    "#     padding = np.ones((label_max_len-len(label))) * (len(OneWord)+1)    # -1  \n",
    "#     return np.concatenate((np.array(label), padding), axis=0)\n",
    "\n",
    "# # def get_unpadded_label(label):\n",
    "# #     padding = np.ones((0)) * -1\n",
    "# #     return np.concatenate((np.array(label), padding), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, shuffle = True):\n",
    "        self.indexes = np.arange(len(txt_train))\n",
    "        self.shuffle = True\n",
    "        self.batch_size = minibatch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(image_train) / minibatch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 生成batch_size个索引\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        if (len(batch_indexes) !=  minibatch_size):\n",
    "            self.batch_size = len(batch_indexes)\n",
    "        else:\n",
    "            self.batch_size = minibatch_size\n",
    "        \n",
    "        batch_data = []\n",
    "        batch_data = image_train[batch_indexes]\n",
    "\n",
    "        batch_label = []\n",
    "        for index1 in txt_train[batch_indexes]:\n",
    "            temp = text_to_labels(index1)\n",
    "            temp = get_padded_label(temp)\n",
    "#             temp = get_unpadded_label(temp)\n",
    "            batch_label.append(temp)\n",
    "        batch_label = np.array(batch_label)\n",
    "        \n",
    "        batch_input_length = []\n",
    "        batch_input_length = train_input_length[batch_indexes]\n",
    "        \n",
    "        batch_label_length = []\n",
    "        batch_label_length = train_label_length[batch_indexes]\n",
    "        \n",
    "        # 畫素資料浮點化以便歸一化\n",
    "        batch_data = batch_data.astype('float32')\n",
    "        batch_data /= 255\n",
    "        \n",
    "        inputs = {'the_input': batch_data,\n",
    "                  'the_labels': batch_label,\n",
    "                  'input_length': batch_input_length,\n",
    "                  'label_length': batch_label_length}\n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "        \n",
    "        return (inputs, outputs)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        #在每一次epoch结束是否需要进行一次随机，重新随机一下index\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValDataGenerator(keras.utils.Sequence):\n",
    "    def __init__(self, shuffle = True):\n",
    "        self.indexes = np.arange(len(txt_val))\n",
    "        self.shuffle = True\n",
    "        self.batch_size = minibatch_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(image_val) / minibatch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 生成batch_size个索引\n",
    "        batch_indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        \n",
    "        if (len(batch_indexes) !=  minibatch_size):\n",
    "            self.batch_size = len(batch_indexes)\n",
    "        else:\n",
    "            self.batch_size = minibatch_size\n",
    "        \n",
    "        batch_data = []\n",
    "        batch_data = image_val[batch_indexes]\n",
    "\n",
    "        batch_label = []\n",
    "        for index1 in txt_val[batch_indexes]:\n",
    "            temp = text_to_labels(index1)\n",
    "            temp = get_padded_label(temp)\n",
    "#             temp = get_unpadded_label(temp)\n",
    "            batch_label.append(temp)\n",
    "        batch_label = np.array(batch_label)\n",
    "        \n",
    "        batch_input_length = []\n",
    "        batch_input_length = val_input_length[batch_indexes]\n",
    "        \n",
    "        batch_label_length = []\n",
    "        batch_label_length = val_label_length[batch_indexes]\n",
    "        \n",
    "        # 畫素資料浮點化以便歸一化\n",
    "        batch_data = batch_data.astype('float32')\n",
    "        batch_data /= 255\n",
    "        \n",
    "        inputs = {'the_input': batch_data,\n",
    "                  'the_labels': batch_label,\n",
    "                  'input_length': batch_input_length,\n",
    "                  'label_length': batch_label_length}\n",
    "        outputs = {'ctc': np.zeros([self.batch_size])}\n",
    "        \n",
    "        return (inputs, outputs)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        #在每一次epoch结束是否需要进行一次随机，重新随机一下index\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models, layers\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, SpatialDropout3D\n",
    "from keras.layers import Convolution3D, MaxPooling3D\n",
    "from keras.layers.convolutional import Conv3D, ZeroPadding3D\n",
    "from keras.layers.wrappers import Bidirectional, TimeDistributed\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import GRU\n",
    "from keras.layers.core import Lambda\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "from multiprocessing import set_start_method, Pool\n",
    "set_start_method('forkserver')\n",
    "# from Word_Error_Rate import WordErrorRate\n",
    "from Letter_Error_Rate import LetterErrorRate\n",
    "from wer import *\n",
    "from decoders import Decoder\n",
    "# from error_rates import ErrorRates\n",
    "# out_size = len(OneWord)+1               # add ctc blank char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ctc_lambda_func(args):\n",
    "    y_pred, labels, input_length, label_length = args\n",
    "    # From Keras example image_ocr.py:\n",
    "    # the 2 is critical here since the first couple outputs of the RNN\n",
    "    # tend to be garbage:\n",
    "#     y_pred = y_pred[:, 2:, :]\n",
    "    y_pred = y_pred[:, :, :]\n",
    "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
    "def CTC(name, args):\n",
    "    return Lambda(ctc_lambda_func, output_shape=(1,), name=name)(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipNet(object):\n",
    "    def __init__(self, img_c=3, img_w=100, img_h=50, frames_n=120, absolute_max_string_len=label_max_len, output_size=len(tok.word_index) + 2):\n",
    "        self.img_c = img_c\n",
    "        self.img_w = img_w\n",
    "        self.img_h = img_h\n",
    "        self.frames_n = frames_n\n",
    "        self.absolute_max_string_len = absolute_max_string_len\n",
    "        self.output_size = output_size\n",
    "        self.build()\n",
    "    \n",
    "    def build(self):\n",
    "        self.input_data = Input(name='the_input', shape=(120,50,100,3), dtype='float32')\n",
    "        \n",
    "        self.zero1 = ZeroPadding3D(padding=(1, 2, 2), name='zero1')(self.input_data)\n",
    "        self.conv1 = Conv3D(32, (3, 5, 5), strides=(1, 2, 2), kernel_initializer='he_normal', name='conv1')(self.zero1)\n",
    "        self.batc1 = BatchNormalization(name='batc1')(self.conv1)\n",
    "        self.actv1 = Activation('relu', name='actv1')(self.batc1)\n",
    "        self.drop1 = SpatialDropout3D(0.5)(self.actv1)\n",
    "        self.maxp1 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max1')(self.drop1)\n",
    "\n",
    "        self.zero2 = ZeroPadding3D(padding=(1, 2, 2), name='zero2')(self.maxp1)\n",
    "        self.conv2 = Conv3D(64, (3, 5, 5), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv2')(self.zero2)\n",
    "        self.batc2 = BatchNormalization(name='batc2')(self.conv2)\n",
    "        self.actv2 = Activation('relu', name='actv2')(self.batc2)\n",
    "        self.drop2 = SpatialDropout3D(0.5)(self.actv2)\n",
    "        self.maxp2 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max2')(self.drop2)\n",
    "\n",
    "        self.zero3 = ZeroPadding3D(padding=(1, 1, 1), name='zero3')(self.maxp2)\n",
    "        self.conv3 = Conv3D(96, (3, 3, 3), strides=(1, 1, 1), kernel_initializer='he_normal', name='conv3')(self.zero3)\n",
    "        self.batc3 = BatchNormalization(name='batc3')(self.conv3)\n",
    "        self.actv3 = Activation('relu', name='actv3')(self.batc3)\n",
    "        self.drop3 = SpatialDropout3D(0.5)(self.actv3)\n",
    "        self.maxp3 = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max3')(self.drop3)\n",
    "\n",
    "        self.resh1 = TimeDistributed(Flatten())(self.maxp3)\n",
    "\n",
    "        self.gru_1 = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(self.resh1)\n",
    "        self.gru_2 = Bidirectional(GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru2'), merge_mode='concat')(self.gru_1)\n",
    "\n",
    "        # transforms RNN output to character activations:\n",
    "        self.dense1 = Dense(self.output_size, kernel_initializer='he_normal', name='dense1')(self.gru_2)\n",
    "\n",
    "        self.y_pred = Activation('softmax', name='softmax')(self.dense1)\n",
    "\n",
    "        self.labels = Input(name='the_labels', shape=[self.absolute_max_string_len], dtype='float32')\n",
    "        self.input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
    "        self.label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
    "\n",
    "        self.loss_out = CTC('ctc', [self.y_pred, self.labels, self.input_length, self.label_length])\n",
    "\n",
    "        self.model = Model(inputs=[self.input_data, self.labels, self.input_length, self.label_length], outputs=self.loss_out)\n",
    "        \n",
    "    def summary(self):\n",
    "        Model(inputs=self.input_data, outputs=self.y_pred).summary()\n",
    "\n",
    "    def predict(self, input_batch):\n",
    "        return self.test_function([input_batch, 0])[0]  # the first 0 indicates test\n",
    "\n",
    "    @property\n",
    "    def test_function(self):\n",
    "        # captures output of softmax so we can decode the output during visualization\n",
    "        return K.function([self.input_data, K.learning_phase()], [self.y_pred])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipnet = LipNet(img_c=3, img_w=100, img_h=50, frames_n=120, absolute_max_string_len=label_max_len, output_size=len(tok.word_index) + 2)\n",
    "lipnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = TrainDataGenerator()\n",
    "val_generator = ValDataGenerator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "lipnet.model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复模型结构及权重\n",
    "# lipnet.model.load_weights('./weight/368-overlap-2.h5')\n",
    "# lipnet.model.load_weights('./weight/lipnet_800_letter.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder = Decoder(greedy=True,beam_width=200,postprocessors=[labels_to_text])\n",
    "# error_rates = ErrorRates(lipnet, val_generator, decoder, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(greedy=True,beam_width=100,postprocessors=[labels_to_text])\n",
    "# error_rates = ErrorRates(lipnet, val_generator, decoder, 256)\n",
    "LetterError_Rate = LetterErrorRate(lipnet, val_generator, decoder, minibatch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lipnet.model.fit_generator(generator = train_generator,\n",
    "                           validation_data = val_generator,\n",
    "                           epochs = 200,                 \n",
    "                           callbacks = [LetterError_Rate],\n",
    "                           verbose = 1,\n",
    "                           max_q_size = 5,\n",
    "                           shuffle = True\n",
    "#                            workers = 2,\n",
    "#                            pickle_safe=True,\n",
    "#                            use_multiprocessing = True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "--------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型结构及权重\n",
    "# lipnet.model.save('./weight/lipnet_1000_letter.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复模型结构及权重\n",
    "# lipnet.model.load_weights('./weight/368-overlap-6.h5')\n",
    "lipnet.model.load_weights('./weight/lipnet_1000_letter.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial = 240             # 0, 120, 240\n",
    "pred_batch = 120\n",
    "def predicts(data, input_length):\n",
    "    batch_data = []\n",
    "    batch_data = data[initial:initial + pred_batch]\n",
    "    \n",
    "    # 畫素資料浮點化以便歸一化\n",
    "    batch_data = batch_data.astype('float32')\n",
    "    batch_data /= 255\n",
    "    \n",
    "    batch_input_length = []\n",
    "    batch_input_length = input_length[initial:initial + pred_batch]\n",
    "    \n",
    "    return (batch_data, batch_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data, pred_input_length = predicts(image_val, val_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lipnet.predict(pred_data)\n",
    "print(y_pred.shape)\n",
    "# print(pred_input_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctc decode\n",
    "r = K.ctc_decode(y_pred, pred_input_length, greedy = True, beam_width=100, top_paths=1)\n",
    "r1 = K.get_value(r[0][0])\n",
    "# print(r1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Decoder(decoded, **kwargs):\n",
    "    postprocessors = kwargs.get('postprocessors', [])\n",
    "    preprocessed = []\n",
    "    for output in decoded:\n",
    "        out = output\n",
    "        for postprocessor in postprocessors:\n",
    "            out = postprocessor(out)\n",
    "        preprocessed.append(out)\n",
    "    return(preprocessed)\n",
    "result = Decoder(r1, postprocessors=[labels_to_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in result:\n",
    "    print('Predict label:', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in txt_val[initial:initial + pred_batch]:\n",
    "    print('True label:', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "def GetEditDistance(str1, str2):\n",
    "    leven_cost = 0\n",
    "    s = difflib.SequenceMatcher(None, str1, str2)\n",
    "    for tag, i1, i2, j1, j2 in s.get_opcodes():\n",
    "        #print('{:7} a[{}: {}] --> b[{}: {}] {} --> {}'.format(tag, i1, i2, j1, j2, str1[i1: i2], str2[j1: j2]))\n",
    "        if tag == 'replace':\n",
    "            leven_cost += max(i2-i1, j2-j1)\n",
    "        elif tag == 'insert':\n",
    "            leven_cost += (j2-j1)\n",
    "        elif tag == 'delete':\n",
    "            leven_cost += (i2-i1)\n",
    "    return leven_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cer(predict, label, label_length):\n",
    "    # print(data)\n",
    "    # mean_length = np.mean([len(d[1]) for d in data])\n",
    "    cha_num = 0\n",
    "    cha_error_num = 0\n",
    "\n",
    "    for i in range(len(predict)):\n",
    "        cha_edit_distance = GetEditDistance(str(predict[i]), str(label[i]))\n",
    "        cha_num = cha_num + label_length[i]\n",
    "\n",
    "        if(cha_edit_distance <= label_length[i]):\n",
    "            cha_error_num += cha_edit_distance\n",
    "        else:\n",
    "            cha_error_num += label_length[i]\n",
    "\n",
    "    return (cha_error_num / cha_num) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wer(predict, label, label_length):\n",
    "    # print(data)\n",
    "    # mean_length = np.mean([len(d[1].split()) for d in data])\n",
    "    words_num = 0\n",
    "    word_error_num = 0\n",
    "\n",
    "    for i in range(len(predict)):\n",
    "        word_edit_distance = chinese_wer_sentence(str(predict[i]), str(label[i]))\n",
    "        words_num = words_num + label_length[i]\n",
    "\n",
    "        if(word_edit_distance <= label_length[i]):\n",
    "            word_error_num += word_edit_distance\n",
    "        else:\n",
    "            word_error_num += label_length[i]\n",
    "\n",
    "    return (word_error_num / words_num) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Letter_length = []\n",
    "Word_length = []\n",
    "for i in txt_val[initial:initial + pred_batch]:\n",
    "    Letter_length.append(len(i))\n",
    "    Word_length.append(len(seg.cut(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer = calculate_wer(result, txt_val[initial:initial + pred_batch], Word_length)\n",
    "cer = calculate_cer(result, txt_val[initial:initial + pred_batch], Letter_length)\n",
    "print(\"wer: \" + str(wer))\n",
    "print(\"cer: \" + str(cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
